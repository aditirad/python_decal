{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "318498f4-7f14-42a7-bbfd-6ef83d3349b7",
      "metadata": {
        "id": "318498f4-7f14-42a7-bbfd-6ef83d3349b7"
      },
      "source": [
        "# INFO 159/259"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e9037f2a-d06a-4c4c-ba71-d571e5873c01",
      "metadata": {
        "id": "e9037f2a-d06a-4c4c-ba71-d571e5873c01"
      },
      "source": [
        "# <center> Homework 1: Word Embeddings </center>\n",
        "<center> Due: February 3, 2026 @ 11:59pm </center>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "47db1e18-852a-46ee-8ae9-c9b1b8682320",
      "metadata": {
        "id": "47db1e18-852a-46ee-8ae9-c9b1b8682320"
      },
      "source": [
        "# HW1: Word Embeddings\n",
        "\n",
        "In this homework, you will implement _word2vec_ with skip-grams and negative sampling, training on a small slice of Wikipedia data.\n",
        "\n",
        "*Learning objectives*:\n",
        "- Understand the implementation details of _word2vec_\n",
        "- Gain familiarity with `numpy` for matrix math\n",
        "- Gain familiarity with training a classifier using stochastic gradient descent.\n",
        "\n",
        "You may want to consult SLP chapter 5 (_Embeddings_) as a reference for the implementation. This homework is designed to run on the CPU only, so if you are using Google Colab, you may want to ensure that your CPU is selected (under `Runtime > Change runtime type` in the top bar) so that you save your GPU allocation for later assignments in the semester."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "6017ce58-b658-47be-a26b-2b08cfa5a696",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6017ce58-b658-47be-a26b-2b08cfa5a696",
        "outputId": "e7f7e4d7-f762-42ec-e4bd-b3c670d51ba3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2026-02-05 20:17:23--  https://github.com/dbamman/nlp-course/raw/refs/heads/main/HW/data/en_wiki_sample.txt\n",
            "Resolving github.com (github.com)... 20.205.243.166\n",
            "Connecting to github.com (github.com)|20.205.243.166|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/dbamman/nlp-course/refs/heads/main/HW/data/en_wiki_sample.txt [following]\n",
            "--2026-02-05 20:17:23--  https://raw.githubusercontent.com/dbamman/nlp-course/refs/heads/main/HW/data/en_wiki_sample.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.108.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 42894174 (41M) [text/plain]\n",
            "Saving to: ‘en_wiki_sample.txt’\n",
            "\n",
            "en_wiki_sample.txt  100%[===================>]  40.91M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2026-02-05 20:17:24 (282 MB/s) - ‘en_wiki_sample.txt’ saved [42894174/42894174]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# download the dataset we will be using\n",
        "!wget https://github.com/dbamman/nlp-course/raw/refs/heads/main/HW/data/en_wiki_sample.txt -O en_wiki_sample.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "32448ff3-a870-4621-9f9d-a1e83f3bc641",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "32448ff3-a870-4621-9f9d-a1e83f3bc641",
        "outputId": "b67dd5b3-7834-4a6f-ff1d-9f4ee034251f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "import itertools\n",
        "from collections import Counter\n",
        "\n",
        "import numpy as np\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from tqdm import tqdm\n",
        "\n",
        "nltk.download(\"punkt_tab\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9b6563cd-75ec-472f-bc73-d0af6fd4e68e",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "9b6563cd-75ec-472f-bc73-d0af6fd4e68e"
      },
      "source": [
        "## Data loading\n",
        "\n",
        "We will begin by loading and tokenizing the data. The file contains a list of paragraphs from Wikipeda, separated by newlines. Because each document (a paragraph) is sampled independently, we want to maintain the document boundaries when we sample contexts later.\n",
        "\n",
        "Inside `FileDataLoader`:\n",
        "- `idx2vocab` is a list of unique word types\n",
        "- `vocab2idx` is a dict mapping from a word type to its index in `idx2vocab`\n",
        "- `word_freqs` is a dict mapping from a word type to its frequency in the corpus\n",
        "\n",
        "You should implement:\n",
        "1. The `negative_sample_weights()` function\n",
        "\n",
        "   This function should calculate the weighted sample probabilities for each of the words in our vocabulary.\n",
        "   Recall SLP3 eq. 5.19:\n",
        "    $$\n",
        "     P_{\\alpha}(w) = \\frac{\\text{count}(w)^{\\alpha}}{\\sum_{w'}\\text{count}(w')^{\\alpha}}\n",
        "    $$\n",
        "   We calculate and store the sample weights to save time when generating contexts later.\n",
        "3. The `negative_sample()` function\n",
        "\n",
        "   This function should sample `num_samples` negative context words given a target word. Recall from SLP3 5.5.2\n",
        "   > A noise word is a random word from the lexicon, **constrained not to be the target word $w$**. (_emph added_)\n",
        "\n",
        "   So, when sampling, you will want to copy the original `.sample_weights` numpy array, set the probability of the target word to 0, and renormalize the weights before sampling.\n",
        "\n",
        "   You may want to consult the numpy documentation for [`numpy.random.Generator.choice()`](https://numpy.org/doc/stable/reference/random/generated/numpy.random.Generator.choice.html#numpy.random.Generator.choice). We have instantiated a random generator for your convenience in `self.rng`."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fe878ae3-b7ff-45a3-9a85-1dcb2a9b001e",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "fe878ae3-b7ff-45a3-9a85-1dcb2a9b001e"
      },
      "source": [
        "_Learning objectives_:\n",
        "> - Understand the implementation details of _word2vec_\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "7411592b-c7af-4fc2-a332-ccd22c87c26e",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "7411592b-c7af-4fc2-a332-ccd22c87c26e"
      },
      "outputs": [],
      "source": [
        "corpus_path = \"./en_wiki_sample.txt\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "df7830a5-b148-4002-b927-73f7bcf3d005",
      "metadata": {
        "tags": [
          "otter_answer_cell"
        ],
        "id": "df7830a5-b148-4002-b927-73f7bcf3d005"
      },
      "outputs": [],
      "source": [
        "class FileDataLoader():\n",
        "    def __init__(self, filepath, negative_sample_alpha=0.75, min_threshold=5):\n",
        "        self.negative_sample_alpha = negative_sample_alpha\n",
        "        self.min_threshold = min_threshold\n",
        "\n",
        "        self.tokenized_documents = self.load_data(filepath)\n",
        "        self.word_freqs = self.get_word_freqs(self.tokenized_documents)\n",
        "\n",
        "        # replace words that appear fewer than min_threshold times with an [UNK] token\n",
        "        for word, freq in list(self.word_freqs.items()):\n",
        "            if freq < min_threshold:\n",
        "                self.word_freqs[\"[UNK]\"] += freq\n",
        "                del self.word_freqs[word]\n",
        "\n",
        "        self.idx2vocab = list(self.word_freqs.keys())\n",
        "        self.vocab2idx = {word: index for index, word in enumerate(self.idx2vocab)}\n",
        "\n",
        "        # set up a random number generator we can use for sampling\n",
        "        self.rng = np.random.default_rng(159259)\n",
        "        self.sample_weights = self.negative_sample_weights(alpha=negative_sample_alpha)\n",
        "\n",
        "        ...\n",
        "\n",
        "    def tokenize_and_lowercase(self, doc):\n",
        "        \"\"\"Tokenize a doc and lowercase all the words.\"\"\"\n",
        "        return [word.lower() for word in word_tokenize(doc)]\n",
        "\n",
        "    def get_word_freqs(self, tokenized_documents):\n",
        "        \"\"\"Return a dictionary mapping each word to its frequency.\"\"\"\n",
        "        return Counter(itertools.chain.from_iterable(tokenized_documents))\n",
        "\n",
        "    def load_data(self, filepath):\n",
        "        return [self.tokenize_and_lowercase(doc) for doc in tqdm(open(corpus_path, \"r\").readlines())]\n",
        "\n",
        "    def negative_sample_weights(self, alpha):\n",
        "        \"\"\"Calculate the weighted probabilities of each word.\n",
        "\n",
        "        Return a (v,)-shaped numpy array, where v is the size of the vocabulary.\n",
        "        \"\"\"\n",
        "        # TODO: implement this function\n",
        "        counts = np.array([self.word_freqs[word] for word in self.idx2vocab])\n",
        "        powered = counts ** alpha\n",
        "        total = powered.sum()\n",
        "        probs = powered / total\n",
        "        return probs\n",
        "\n",
        "    def negative_sample(self, target_word_idx, num_samples):\n",
        "        \"\"\"Sample num_samples noise words from the lexicon that is not the target word.\n",
        "\n",
        "        The sample probabilities should be proportional to their weighted unigram probability if the target word probability is set to 0.\n",
        "\n",
        "        Return a (num_samples,)-shaped numpy array of sampled indices.\n",
        "        \"\"\"\n",
        "        # TODO: implement this function\n",
        "        probs = self.sample_weights.copy()\n",
        "        probs[target_word_idx] = 0.0\n",
        "\n",
        "        # Renormalize\n",
        "        total = probs.sum()\n",
        "        if total > 0:\n",
        "            probs = probs / total\n",
        "        else:\n",
        "            # Fallback: extremely rare case — uniform over all except target\n",
        "            probs = np.ones_like(probs)\n",
        "            probs[target_word_idx] = 0.0\n",
        "            probs = probs / probs.sum()\n",
        "\n",
        "        samples = self.rng.choice(\n",
        "            a=len(probs),\n",
        "            size=num_samples,\n",
        "            replace=True,\n",
        "            p=probs\n",
        "        )\n",
        "        return samples\n",
        "\n",
        "    def sample_contexts(self, window_size, sample_k):\n",
        "        for doc in self.tokenized_documents:\n",
        "            if len(doc) < (2 * window_size) + 1:\n",
        "                # the doc is too short for our desired window size; we skip it\n",
        "                continue\n",
        "            for word_idx in range(window_size, len(doc) - window_size):\n",
        "                target_word_idx = self.vocab2idx[doc[word_idx]] if doc[word_idx] in self.vocab2idx else self.vocab2idx[\"[UNK]\"]\n",
        "                # sample positive words from the window\n",
        "                positive_word_idxs = np.array([\n",
        "                    self.vocab2idx[word] if word in self.vocab2idx else self.vocab2idx[\"[UNK]\"] for word in doc[word_idx - window_size:word_idx] + doc[word_idx + 1:word_idx + 1 + window_size]\n",
        "\n",
        "                ])\n",
        "                # sample len(positive_word_idxs) * sample_k number of negative words\n",
        "                negative_word_idxs = self.negative_sample(target_word_idx, sample_k * len(positive_word_idxs))\n",
        "                yield (target_word_idx, positive_word_idxs, negative_word_idxs)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "ee0d413e-3180-4601-9e6a-3ce2c9cbe07b",
      "metadata": {
        "deletable": false,
        "editable": false,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ee0d413e-3180-4601-9e6a-3ce2c9cbe07b",
        "outputId": "8b90d045-c47c-4ddc-c4a0-ceaf1db93b42"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100000/100000 [00:48<00:00, 2058.17it/s]\n"
          ]
        }
      ],
      "source": [
        "# this should take roughly 30 seconds\n",
        "dataloader = FileDataLoader(corpus_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8c4c9e9e-9515-4283-92e6-2aad6065e023",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "8c4c9e9e-9515-4283-92e6-2aad6065e023"
      },
      "source": [
        "**Quick check**: The unweighted probability for \"the\" should be 0.063; the weighted probability should be 0.016."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "82045973-aa69-4fce-b055-db8d9bbdd47e",
      "metadata": {
        "deletable": false,
        "editable": false,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "82045973-aa69-4fce-b055-db8d9bbdd47e",
        "outputId": "b07b05da-03ae-4087-b407-17081e2fe4f7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unweighted probability for `the`: \t\t0.063\n",
            "Weighted (alpha=0.75) probability for `the`: \t0.016\n"
          ]
        }
      ],
      "source": [
        "print(f\"Unweighted probability for `the`: \\t\\t{dataloader.word_freqs['the'] / dataloader.word_freqs.total():.3f}\")\n",
        "print(f\"Weighted (alpha=0.75) probability for `the`: \\t{dataloader.sample_weights[dataloader.vocab2idx['the']]:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "77ac31be-c0cf-4fe3-a6e1-5494f4c596d2",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "77ac31be-c0cf-4fe3-a6e1-5494f4c596d2"
      },
      "source": [
        "## Setting up the model\n",
        "\n",
        "The word2vec model consists of two matrices: the target (or input) embedding and the context (or output) embedding. We set those up here.\n",
        "\n",
        "You should implement:\n",
        "- The `nearest_neighbors()` function\n",
        "\n",
        "  This given a $d$-dimensional $\\vec{v}$ and a $(v \\times d)$-dimensional matrix $M$ of vectors to query against, we want to calculate the cosine similarity of $\\vec{v}$ with each row of $M$ and return the indices (and the corresponding similarities) of the most similar rows in $M$.\n",
        "\n",
        "  As a reminder, the cosine similarity of two vectors $\\vec{a}$ and $\\vec{b}$ is\n",
        "  $$\n",
        "    \\text{cosine\\_sim}(\\vec{a}, \\vec{b}) = \\frac{\\vec{a} \\cdot \\vec{b}}{\\|{\\vec{a}}\\|\\|\\vec{b}\\|}\n",
        "  $$\n",
        "\n",
        "  This is derived from one of the formulations for the dot product:\n",
        "  $$\n",
        "    \\vec{a} \\cdot \\vec{b} = \\|\\vec{a}\\| \\|\\vec{b}\\| \\cos({\\theta})\n",
        "  $$\n",
        "\n",
        "  $\\|\\vec{a}\\|$ denotes the $l_2$-norm of a vector, or its magnitude.\n",
        "\n",
        "  You might want to consult the numpy documentation for [`numpy.matmul`](https://numpy.org/doc/2.1/reference/generated/numpy.matmul.html), [`numpy.argsort`](https://numpy.org/doc/2.1/reference/generated/numpy.argsort.html#numpy-argsort), and [`numpy.linalg.norm`](https://numpy.org/doc/2.1/reference/generated/numpy.linalg.norm.html)\n",
        "\n",
        "\n",
        "_Learning objectives_:\n",
        "> - Gain familiarity with `numpy` for matrix math\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "786c582e-c88c-45d0-944f-3cf2b6e165bf",
      "metadata": {
        "tags": [
          "otter_answer_cell"
        ],
        "id": "786c582e-c88c-45d0-944f-3cf2b6e165bf"
      },
      "outputs": [],
      "source": [
        "class Word2Vec():\n",
        "    def __init__(self, dataloader, hidden_dim=100):\n",
        "        self.dataloader = dataloader\n",
        "        self.vocab_size = len(self.dataloader.idx2vocab)\n",
        "        self.hidden_dim = hidden_dim\n",
        "\n",
        "        np.random.seed(159259)\n",
        "        # We initialize the model weights to be uniformly randomly distributed and centered around 0.\n",
        "        self.target_embs = (np.random.random((self.vocab_size, hidden_dim)) - 0.5) / hidden_dim\n",
        "        self.context_embs = (np.random.random((self.vocab_size, hidden_dim)) - 0.5) / hidden_dim\n",
        "\n",
        "    def nearest_neighbors(self, query_vector, vectors, n=10):\n",
        "        \"\"\"Finds the `n` indices of the rows in `vectors` that have the highest cosine similarity to `query_vector`.\n",
        "\n",
        "        query_vector: (d,)-shaped numpy array\n",
        "        vectors: (v, d)-shaped numpy array\n",
        "        n: int\n",
        "\n",
        "        Return a tuple of (indices, similarities), where both are (n,)-shaped ndarrays.\n",
        "        \"\"\"\n",
        "        # Normalize query vector\n",
        "        query_norm = query_vector / (np.linalg.norm(query_vector) + 1e-10)\n",
        "\n",
        "        # Normalize all rows in vectors (matrix)\n",
        "        norms = np.linalg.norm(vectors, axis=1, keepdims=True)\n",
        "        norms[norms == 0] = 1e-10  # prevent division by zero\n",
        "        normalized_vectors = vectors / norms\n",
        "\n",
        "        # Cosine similarity = dot product of normalized vectors\n",
        "        similarities = np.dot(normalized_vectors, query_norm)\n",
        "\n",
        "        # Get indices of top n largest similarities (descending order)\n",
        "        top_indices = np.argsort(similarities)[::-1][:n]\n",
        "        top_similarities = similarities[top_indices]\n",
        "\n",
        "        return top_indices, top_similarities\n",
        "\n",
        "    def print_nearest_neighbors(self, word, n=5):\n",
        "        \"\"\"Prints the `n` nearest neighbors for a word using the context embeddings.\n",
        "\n",
        "        word: str\n",
        "\n",
        "        Return None\n",
        "        \"\"\"\n",
        "        query_vector = self.context_embs[self.dataloader.vocab2idx[word]]\n",
        "        closest_inds, similarities = self.nearest_neighbors(query_vector, self.context_embs, n)\n",
        "        words = [self.dataloader.idx2vocab[ind] for ind in closest_inds]\n",
        "\n",
        "        print(words)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "c86a648f-9671-4f85-8644-a095f9dfe708",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "c86a648f-9671-4f85-8644-a095f9dfe708"
      },
      "outputs": [],
      "source": [
        "w2v_model = Word2Vec(dataloader)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cf8bc322-4dad-4810-b4c1-eea1298c4042",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "cf8bc322-4dad-4810-b4c1-eea1298c4042"
      },
      "source": [
        "**Quick check**: you can check your function against this toy example. The output should be:\n",
        "\n",
        "- `(array([4, 5, 0, 6, 3]), array([0.91347529, 0.87409283, 0.84518755, 0.83396453, 0.8111933 ]))`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "c3a58b3e-b51b-423e-bf1e-71764ef0d9fb",
      "metadata": {
        "deletable": false,
        "editable": false,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c3a58b3e-b51b-423e-bf1e-71764ef0d9fb",
        "outputId": "4550773d-bc3d-46c2-f2bc-fc77aac313f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(array([4, 5, 0, 6, 3]), array([0.91347529, 0.87409283, 0.84518755, 0.83396453, 0.8111933 ]))\n"
          ]
        }
      ],
      "source": [
        "def quick_check():\n",
        "    np.random.seed(159259)\n",
        "    query_vec = np.random.random(size=(5,))\n",
        "    other_vecs = np.random.random(size=(10, 5))\n",
        "    print(w2v_model.nearest_neighbors(query_vec, other_vecs, n=5))\n",
        "\n",
        "quick_check()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cd6196d4-bf38-4de1-b3fd-6ca986056a7a",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "cd6196d4-bf38-4de1-b3fd-6ca986056a7a"
      },
      "source": [
        "**Quick check**: the nearest neighbors for \"the\" should be random at this point; if you did not edit the `__init__` function, the nearest neighbors should be:\n",
        "\n",
        "- `['the', 'asian', 'habilitation', 'toward', 'capacity-building']`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "bcd4e8e8-10a2-4a02-8fcd-cf05d63624b0",
      "metadata": {
        "deletable": false,
        "editable": false,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bcd4e8e8-10a2-4a02-8fcd-cf05d63624b0",
        "outputId": "e948c42a-2cc5-42b4-bee5-f0b671408a83"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['the', 'asian', 'habilitation', 'toward', 'capacity-building']\n"
          ]
        }
      ],
      "source": [
        "w2v_model.print_nearest_neighbors(\"the\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "79dab544-e659-434e-8ef0-74b620cccc9d",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "79dab544-e659-434e-8ef0-74b620cccc9d"
      },
      "source": [
        "## Setting up the training loop"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "77f9f4b2-9e29-4d62-8d5e-0e04797fd2b1",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "77f9f4b2-9e29-4d62-8d5e-0e04797fd2b1"
      },
      "source": [
        "### Calculating gradients\n",
        "\n",
        "To update the weights using gradient descent, we have to find the partial derivatives of the loss with respect to the parameters. You can find the loss function and its partial derivatives in SLP 5.5.2 (eqs. 5.22 - 5.24); we've also reproduced them for you below. While we give you the derivatives, it can be a good exercise to try to derive them yourself!\n",
        "\n",
        "These rely on the sigmoid function, which we've implemented for you as an example.\n",
        "\n",
        "You should implement:\n",
        "- `loss_fn`\n",
        "- `c_pos_grad`\n",
        "- `c_neg_grad`\n",
        "- `w_grad`\n",
        "\n",
        "In each of these functions, you should expect:\n",
        "- `w` to be a `d`-dimensional vector,\n",
        "- `c_pos` to be a `(n_pos, d)`-dimensional matrix (where `n_pos` is the number of positive context examples)\n",
        "- `c_neg` to be a `(n_neg, d)`-dimensional matrix (where `n_neg` is the number of negative context examples)\n",
        "\n",
        "As a reminder, the sigmoid function is defined as\n",
        "$$\n",
        "\\sigma(x) = \\frac{1}{1 + e^{-x}}\n",
        "$$\n",
        "\n",
        "For filling out the rest of the functions, you may want to use [`np.log`](https://numpy.org/devdocs/reference/generated/numpy.log.html#numpy.log), [`np.sum`](https://numpy.org/devdocs/reference/generated/numpy.sum.html), [`np.newaxis`](https://numpy.org/devdocs/reference/constants.html#numpy.newaxis), [`np.matmul`](https://numpy.org/devdocs/reference/generated/numpy.matmul.html#numpy-matmul), and of course, the `sigmoid` function that we have implemented for you."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "ca4fe294-5650-4bca-a43a-787ac3df9125",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "ca4fe294-5650-4bca-a43a-787ac3df9125"
      },
      "outputs": [],
      "source": [
        "# we wrap these functions in the @njit decorator to speed up calculations\n",
        "# using just-in-time compilation\n",
        "# you don't have to worry about this\n",
        "from numba import njit\n",
        "\n",
        "@njit\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "f07f23e3-aea8-4e7e-8fd9-7e163082463a",
      "metadata": {
        "tags": [
          "otter_answer_cell"
        ],
        "id": "f07f23e3-aea8-4e7e-8fd9-7e163082463a"
      },
      "outputs": [],
      "source": [
        "@njit\n",
        "def loss_fn(w, c_pos, c_neg):\n",
        "    pos_scores = c_pos @ w\n",
        "    neg_scores = c_neg @ w\n",
        "\n",
        "    # small epsilon prevents log(0) when sigmoid ≈ 1\n",
        "    pos_loss = -np.log(sigmoid(pos_scores) + 1e-12)\n",
        "    neg_loss = -np.log(1.0 - sigmoid(neg_scores) + 1e-12)\n",
        "\n",
        "    return np.sum(pos_loss) + np.sum(neg_loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "62d4d438-55d4-4679-bcd6-085eb8a4e0b7",
      "metadata": {
        "tags": [
          "otter_answer_cell"
        ],
        "id": "62d4d438-55d4-4679-bcd6-085eb8a4e0b7"
      },
      "outputs": [],
      "source": [
        "@njit\n",
        "def c_pos_grad(w, c_pos):\n",
        "    # Gradient w.r.t. positive context embeddings\n",
        "    # Shape: (n_pos, d)\n",
        "    scores = c_pos @ w                  # (n_pos,)\n",
        "    sig = sigmoid(scores)\n",
        "    delta = sig - 1.0                   # (n_pos,)\n",
        "\n",
        "    return delta[:, np.newaxis] * w     # (n_pos, d)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "b76a8d2c-f760-48c4-bef3-b39de087d9b2",
      "metadata": {
        "tags": [
          "otter_answer_cell"
        ],
        "id": "b76a8d2c-f760-48c4-bef3-b39de087d9b2"
      },
      "outputs": [],
      "source": [
        "@njit\n",
        "def c_neg_grad(w, c_neg):\n",
        "   # Gradient w.r.t. negative context embeddings\n",
        "    # Shape: (n_neg, d)\n",
        "    scores = c_neg @ w\n",
        "    sig = sigmoid(scores)\n",
        "\n",
        "    return sig[:, np.newaxis] * w       # (n_neg, d)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "e99ed4d6-fff8-4984-9a02-4da80a63eb97",
      "metadata": {
        "tags": [
          "otter_answer_cell"
        ],
        "id": "e99ed4d6-fff8-4984-9a02-4da80a63eb97"
      },
      "outputs": [],
      "source": [
        "@njit\n",
        "def w_grad(w, c_pos, c_neg):\n",
        "    pos_scores = c_pos @ w\n",
        "    neg_scores = c_neg @ w\n",
        "\n",
        "    pos_sig = sigmoid(pos_scores)\n",
        "    neg_sig = sigmoid(neg_scores)\n",
        "\n",
        "    grad_pos = c_pos.T @ (pos_sig - 1.0)   # (d,)\n",
        "    grad_neg = c_neg.T @ neg_sig           # (d,)\n",
        "\n",
        "    return grad_pos + grad_neg"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fd707bf6-2271-41ae-a2d8-219b69fe5c3f",
      "metadata": {
        "deletable": false,
        "editable": false,
        "jp-MarkdownHeadingCollapsed": true,
        "id": "fd707bf6-2271-41ae-a2d8-219b69fe5c3f"
      },
      "source": [
        "**(Not so) Quick check**: We can check the correctness of the loss function and gradient calculations by numerically approximating the gradients using neighboring points and seeing if they match up. Recall from your calculus class:\n",
        "\n",
        "$$\n",
        "\\frac{d}{dx} f(x) = \\lim_{h \\to 0} \\frac{f(x + h) - f(x - h)}{2h}\n",
        "$$\n",
        "\n",
        "We implement this in the `approximate_gradient` function so that we can estimate the local gradient and see if the closed-form solution that you implemented in the functions above are accurate. However, we never numerically approximate the gradient during training because we have a closed-form solution that is both more accurate and more efficient to calculate.\n",
        "\n",
        "> **Aside**: In this assignment, we have you manually calculate the loss and gradients. If you have taken other deep learning classes, you may have experience with libraries like Pytorch, which implement automatic differentiation so that you can just specify the loss function and not have to work out the gradients manually.\n",
        ">\n",
        "> These libraries _don't_ use numerical approximation for the gradients. Instead, they rely on the chain rule:\n",
        ">\n",
        "> $$\n",
        "    \\frac{d}{dx} f(g(x)) = f'(g(x)) g'(x)\n",
        "  $$\n",
        "> As long as all of the functions you apply to an input are differentiable, and the closed-form derivatives are known (which they often are, since most functions break down into basic differentiable operations like addition, multiplication, or exponentiation), the library can construct a graph to track all of the applications of the functions and calculate the partial derivatives using this graph.\\\n",
        ">\n",
        "> You can read more about this in the [Pytorch autograd tutorial](https://docs.pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html#computational-graph).\n",
        "\n",
        "Your loss should be roughly 8.05; if it is not, all of the assertions in the `quick_check` will likely fail even if (especially if) your gradients are implemented correctly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "4fecb738-819b-42d3-a417-8ef633c3864f",
      "metadata": {
        "deletable": false,
        "editable": false,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4fecb738-819b-42d3-a417-8ef633c3864f",
        "outputId": "a2f6dbb1-d355-4384-ec94-065deb3b2491"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 8.052986383589216\n"
          ]
        }
      ],
      "source": [
        "def quick_check():\n",
        "    np.random.seed(159259)\n",
        "\n",
        "    w = np.random.random((5,))\n",
        "    c_pos = np.random.random((2, 5))\n",
        "    c_neg = np.random.random((4, 5))\n",
        "\n",
        "    eps = 1e-5\n",
        "\n",
        "    def approximate_gradient(func, vec, eps=1e-5):\n",
        "        est_grad = np.zeros(vec.shape)\n",
        "        for ind, el in np.ndenumerate(vec):\n",
        "            perturb = np.zeros(vec.shape)\n",
        "            perturb[ind] = eps\n",
        "            est_grad[ind] = (func(vec + perturb) - func(vec - perturb)) / (2 * eps)\n",
        "        return est_grad\n",
        "\n",
        "    print(\"loss:\", loss_fn(w, c_pos, c_neg))\n",
        "\n",
        "    assert np.allclose(w_grad(w, c_pos, c_neg), approximate_gradient(lambda x: loss_fn(x, c_pos, c_neg), w)), \"c_pos_grad is not correct for loss_fn\"\n",
        "    assert np.allclose(c_pos_grad(w, c_pos), approximate_gradient(lambda x: loss_fn(w, x, c_neg), c_pos)), \"c_pos_grad is not correct for loss_fn\"\n",
        "    assert np.allclose(c_neg_grad(w, c_neg), approximate_gradient(lambda x: loss_fn(w, c_pos, x), c_neg)), \"c_neg_grad is not correct for loss_fn\"\n",
        "\n",
        "quick_check()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7fd3d896-dace-4d50-b0c3-f065af19305b",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "7fd3d896-dace-4d50-b0c3-f065af19305b"
      },
      "source": [
        "### Updating weights in the training loop\n",
        "\n",
        "The training loop for SGD consists of sampling one instance of the data (in our case, a target word and its positive and negative contexts), and calculating the partial derivatives of the loss.\n",
        "\n",
        "We then update the parameters using these partial derivatives, multiplying each gradient by the learning rate $\\eta$. When we perform gradient descent, we subtract the gradients from the weights in order to shift the weights in a direction that decreases the loss (locally, at least). Here are the updates we make:\n",
        "$$\n",
        "c_{\\text{pos}}^{t + 1} = c_{\\text{pos}}^{t} - \\eta \\frac{\\partial L}{\\partial {c}_{\\text{pos}}^t},\n",
        "$$\n",
        "$$\n",
        "c_{\\text{neg}}^{t + 1} = c_{\\text{neg}}^{t} - \\eta \\frac{\\partial L}{\\partial {c}_{\\text{neg}}^t}\n",
        ",$$\n",
        "$$\n",
        "w^{t + 1} = w^{t} - \\eta \\frac{\\partial L}{\\partial w^t}\n",
        ",$$\n",
        "where $t + 1$ is the next timestep in the stochastic gradient descent loop.\n",
        "\n",
        "**Note**: We print some diagnostic information, including the loss, to help you monitor the training. You should convince yourself that, though we calculate the loss and print it here to track our training, SGD doesn't actually require that we compute the loss as such; we really only need the gradients.\n",
        "\n",
        "You implement:\n",
        "- the section of the code where you calculate the gradients\n",
        "- the section of the code where you use the gradients to update the embedding\n",
        "\n",
        "You may want to read about [numpy indexing](https://numpy.org/doc/2.2/user/basics.indexing.html#), since the `.sample_contexts()` returns lists of indices; you might also want to look into [`np.subtract.at()`](https://numpy.org/doc/2.2/reference/generated/numpy.ufunc.at.html) (see the usage of `np.add.at()` in the starter code as another example).\n",
        "\n",
        "With a learning rate of 0.01, you should see some nearest neighbors start to make sense after about the loss drops under 60 or so. This took around 60K steps and 1m21s on our solution code; we recommend running for at least 10 minutes."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8a9ac359-0e50-4a96-8544-4bcf4ce2700a",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "8a9ac359-0e50-4a96-8544-4bcf4ce2700a"
      },
      "source": [
        "_Learning objectives_:\n",
        "> - Gain familiarity with training a classifier using stochastic gradient descent.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "f6a656a1-706f-46e2-8ecf-0f684fd04e3c",
      "metadata": {
        "tags": [
          "otter_answer_cell"
        ],
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "f6a656a1-706f-46e2-8ecf-0f684fd04e3c",
        "outputId": "b5759d04-b0bb-40bc-b378-1fd42efc2762"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "1it [00:00,  5.08it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "avg loss: \n",
            "`he` was updated 0 times in target and 0 times in context\n",
            "['he', 'transponders', 'dusty', 'lse']\n",
            "`original` was updated 0 times in target and 0 times in context\n",
            "['original', 'quivering', 'saltire', 'bae']\n",
            "`january` was updated 0 times in target and 0 times in context\n",
            "['january', 'dailey', 'neutron', 'apogee']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "10025it [00:19, 423.03it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "avg loss: 168.7864973860151\n",
            "`he` was updated 62 times in target and 10654 times in context\n",
            "['he', 'with', 'is', 'at']\n",
            "`original` was updated 0 times in target and 1042 times in context\n",
            "['original', 'over', 'can', 'no']\n",
            "`january` was updated 5 times in target and 1656 times in context\n",
            "['january', ';', 'not', 'two']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "20001it [00:38, 396.95it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "avg loss: 99.46171313075446\n",
            "`he` was updated 115 times in target and 21319 times in context\n",
            "['he', 'his', 'is', 'for']\n",
            "`original` was updated 3 times in target and 2099 times in context\n",
            "['original', 'under', 'i', 'them']\n",
            "`january` was updated 9 times in target and 3354 times in context\n",
            "['january', 'house', '%', 'through']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "30008it [00:57, 233.13it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "avg loss: 80.74189581539237\n",
            "`he` was updated 167 times in target and 31915 times in context\n",
            "['he', 'is', 'that', 'has']\n",
            "`original` was updated 4 times in target and 3109 times in context\n",
            "['original', 'great', 'film', ']']\n",
            "`january` was updated 18 times in target and 4958 times in context\n",
            "['january', '2010', 'there', 'war']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "40003it [01:15, 444.42it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "avg loss: 66.31472379247688\n",
            "`he` was updated 240 times in target and 42555 times in context\n",
            "['he', 'his', 'which', '%']\n",
            "`original` was updated 7 times in target and 4142 times in context\n",
            "['original', 'final', 'black', 'language']\n",
            "`january` was updated 25 times in target and 6531 times in context\n",
            "['january', 'may', 'but', 'which']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "50022it [01:33, 429.36it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "avg loss: 63.57901774861095\n",
            "`he` was updated 295 times in target and 53255 times in context\n",
            "['he', 'was', 'his', 'its']\n",
            "`original` was updated 10 times in target and 5189 times in context\n",
            "['original', 'role', 'society', 'east']\n",
            "`january` was updated 29 times in target and 8154 times in context\n",
            "['january', 'november', '2019', 'even']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "60004it [01:53, 436.58it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "avg loss: 59.129581633381804\n",
            "`he` was updated 358 times in target and 63866 times in context\n",
            "['he', 'it', 'his', 'was']\n",
            "`original` was updated 11 times in target and 6230 times in context\n",
            "['original', 'central', 'society', 'process']\n",
            "`january` was updated 32 times in target and 9829 times in context\n",
            "['january', 'december', 'april', '2021']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "69976it [02:11, 616.25it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "avg loss: 53.30099826618705\n",
            "`he` was updated 423 times in target and 74495 times in context\n",
            "['he', 'it', 'which', 'this']\n",
            "`original` was updated 13 times in target and 7246 times in context\n",
            "['original', 'central', 'football', 'study']\n",
            "`january` was updated 38 times in target and 11503 times in context\n",
            "['january', 'april', 'may', 'september']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "80002it [02:30, 223.17it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "avg loss: 49.564571632336936\n",
            "`he` was updated 472 times in target and 84873 times in context\n",
            "['he', 'it', 'also', 'this']\n",
            "`original` was updated 15 times in target and 8362 times in context\n",
            "['original', 'division', 'act', 'make']\n",
            "`january` was updated 43 times in target and 13165 times in context\n",
            "['january', 'later', 'years', 'these']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "90023it [02:49, 414.15it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "avg loss: 45.449508167910146\n",
            "`he` was updated 524 times in target and 95370 times in context\n",
            "['he', 'it', 'she', 'this']\n",
            "`original` was updated 22 times in target and 9377 times in context\n",
            "['original', 'royal', 'population', 'order']\n",
            "`january` was updated 49 times in target and 14790 times in context\n",
            "['january', 'december', 'october', 'july']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "99999it [03:07, 608.21it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "avg loss: 45.56065816814211\n",
            "`he` was updated 581 times in target and 105966 times in context\n",
            "['he', 'it', 'this', 'she']\n",
            "`original` was updated 26 times in target and 10471 times in context\n",
            "['original', 'end', 'army', 'most']\n",
            "`january` was updated 52 times in target and 16370 times in context\n",
            "['january', 'september', 'november', 'february']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "110000it [03:26, 600.41it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "avg loss: 44.76511420894129\n",
            "`he` was updated 617 times in target and 116489 times in context\n",
            "['he', 'she', 'it', 'also']\n",
            "`original` was updated 31 times in target and 11506 times in context\n",
            "['original', 'support', 'society', 'head']\n",
            "`january` was updated 59 times in target and 17951 times in context\n",
            "['january', 'december', 'may', 'november']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "119997it [03:44, 607.85it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "avg loss: 43.18814245860986\n",
            "`he` was updated 682 times in target and 127176 times in context\n",
            "['he', 'it', 'they', 'also']\n",
            "`original` was updated 34 times in target and 12551 times in context\n",
            "['original', 'role', 'end', 'construction']\n",
            "`january` was updated 61 times in target and 19555 times in context\n",
            "['january', 'october', 'april', 'while']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "129996it [04:04, 419.62it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "avg loss: 40.15894477202373\n",
            "`he` was updated 747 times in target and 137792 times in context\n",
            "['he', 'it', 'was', 'she']\n",
            "`original` was updated 36 times in target and 13617 times in context\n",
            "['original', 'side', 'east', 'head']\n",
            "`january` was updated 70 times in target and 21162 times in context\n",
            "['january', 'september', 'november', 'december']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "139987it [04:22, 606.45it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "avg loss: 39.36942717098174\n",
            "`he` was updated 805 times in target and 148711 times in context\n",
            "['he', 'it', 'they', 'she']\n",
            "`original` was updated 38 times in target and 14652 times in context\n",
            "['original', 'front', 'goal', 'marriage']\n",
            "`january` was updated 71 times in target and 22787 times in context\n",
            "['january', 'september', 'october', 'november']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "149993it [04:40, 614.07it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "avg loss: 38.31133240433373\n",
            "`he` was updated 852 times in target and 159185 times in context\n",
            "['he', 'it', 'also', 'was']\n",
            "`original` was updated 40 times in target and 15678 times in context\n",
            "['original', 'side', 'role', 'front']\n",
            "`january` was updated 79 times in target and 24456 times in context\n",
            "['january', 'december', 'september', 'april']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "159998it [05:00, 606.35it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "avg loss: 38.095661517521336\n",
            "`he` was updated 910 times in target and 169813 times in context\n",
            "['he', 'she', 'it', 'also']\n",
            "`original` was updated 42 times in target and 16779 times in context\n",
            "['original', 'royal', 'role', 'start']\n",
            "`january` was updated 87 times in target and 26137 times in context\n",
            "['january', 'september', 'december', 'october']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "169977it [05:18, 622.39it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "avg loss: 38.328657117761914\n",
            "`he` was updated 969 times in target and 180404 times in context\n",
            "['he', 'it', 'she', 'also']\n",
            "`original` was updated 45 times in target and 17785 times in context\n",
            "['original', 'latter', 'role', 'division']\n",
            "`january` was updated 95 times in target and 27685 times in context\n",
            "['january', 'august', 'december', 'october']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "179991it [05:38, 565.45it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "avg loss: 36.58525935536353\n",
            "`he` was updated 1006 times in target and 190863 times in context\n",
            "['he', 'it', 'she', 'they']\n",
            "`original` was updated 45 times in target and 18816 times in context\n",
            "['original', 'southern', 'finish', 'next']\n",
            "`january` was updated 103 times in target and 29265 times in context\n",
            "['january', 'november', 'august', 'july']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "189969it [05:55, 619.03it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "avg loss: 36.553829205456815\n",
            "`he` was updated 1057 times in target and 201365 times in context\n",
            "['he', 'it', 'she', 'they']\n",
            "`original` was updated 46 times in target and 19872 times in context\n",
            "['original', 'society', 'commission', 'administrative']\n",
            "`january` was updated 105 times in target and 30867 times in context\n",
            "['january', 'december', 'october', 'september']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "199961it [06:14, 428.39it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "avg loss: 35.4696066829595\n",
            "`he` was updated 1107 times in target and 211747 times in context\n",
            "['he', 'it', 'she', 'they']\n",
            "`original` was updated 46 times in target and 20937 times in context\n",
            "['original', 'primary', 'police', 'administration']\n",
            "`january` was updated 110 times in target and 32460 times in context\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "200042it [06:14, 249.73it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['january', 'september', 'december', 'october']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "210023it [06:33, 432.70it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "avg loss: 34.65169144193829\n",
            "`he` was updated 1183 times in target and 222411 times in context\n",
            "['he', 'it', 'she', 'after']\n",
            "`original` was updated 47 times in target and 21975 times in context\n",
            "['original', 'tour', 'police', 'primary']\n",
            "`january` was updated 118 times in target and 34084 times in context\n",
            "['january', 'september', 'december', 'october']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "220009it [06:51, 442.65it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "avg loss: 34.42011707709473\n",
            "`he` was updated 1229 times in target and 232747 times in context\n",
            "['he', 'it', 'she', 'after']\n",
            "`original` was updated 52 times in target and 23032 times in context\n",
            "['original', 'crew', 'police', 'band']\n",
            "`january` was updated 121 times in target and 35710 times in context\n",
            "['january', 'september', 'december', 'october']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "229971it [07:11, 602.66it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "avg loss: 33.491829121224725\n",
            "`he` was updated 1290 times in target and 243328 times in context\n",
            "['he', 'it', 'she', 'after']\n",
            "`original` was updated 58 times in target and 24122 times in context\n",
            "['original', 'hospital', 'band', 'fourth']\n",
            "`january` was updated 127 times in target and 37316 times in context\n",
            "['january', 'september', 'december', 'october']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "240001it [07:29, 447.11it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "avg loss: 34.30835913276074\n",
            "`he` was updated 1352 times in target and 253904 times in context\n",
            "['he', 'it', 'she', 'there']\n",
            "`original` was updated 59 times in target and 25143 times in context\n",
            "['original', 'process', 'fourth', 'match']\n",
            "`january` was updated 131 times in target and 38882 times in context\n",
            "['january', 'september', 'december', 'november']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "249987it [07:47, 340.65it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "avg loss: 32.52467289011175\n",
            "`he` was updated 1404 times in target and 264535 times in context\n",
            "['he', 'it', 'she', 'they']\n",
            "`original` was updated 62 times in target and 26207 times in context\n",
            "['original', 'police', 'fourth', 'straight']\n",
            "`january` was updated 133 times in target and 40482 times in context\n",
            "['january', 'december', 'april', 'march']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "260003it [08:07, 449.76it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "avg loss: 33.53477252233783\n",
            "`he` was updated 1466 times in target and 275224 times in context\n",
            "['he', 'she', 'it', 'they']\n",
            "`original` was updated 68 times in target and 27254 times in context\n",
            "['original', 'previous', 'construction', 'country']\n",
            "`january` was updated 137 times in target and 42122 times in context\n",
            "['january', 'september', 'july', 'december']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "269978it [08:24, 617.63it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "avg loss: 31.533820095626037\n",
            "`he` was updated 1522 times in target and 285724 times in context\n",
            "['he', 'she', 'it', 'they']\n",
            "`original` was updated 71 times in target and 28343 times in context\n",
            "['original', 'largest', 'construction', 'laws']\n",
            "`january` was updated 140 times in target and 43698 times in context\n",
            "['january', 'september', 'december', 'august']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "279978it [08:44, 619.89it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "avg loss: 31.996058609043963\n",
            "`he` was updated 1564 times in target and 296331 times in context\n",
            "['he', 'it', 'she', 'they']\n",
            "`original` was updated 72 times in target and 29390 times in context\n",
            "['original', 'british', 'construction', 'catholic']\n",
            "`january` was updated 143 times in target and 45331 times in context\n",
            "['january', 'december', 'november', 'july']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "289989it [09:02, 621.63it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "avg loss: 31.57539971674375\n",
            "`he` was updated 1616 times in target and 307198 times in context\n",
            "['he', 'she', 'it', 'they']\n",
            "`original` was updated 76 times in target and 30389 times in context\n",
            "['original', 'turn', 'countries', 'western']\n",
            "`january` was updated 148 times in target and 46976 times in context\n",
            "['january', 'july', 'august', 'february']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "300001it [09:24, 154.10it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "avg loss: 30.71744563332779\n",
            "`he` was updated 1687 times in target and 317862 times in context\n",
            "['he', 'she', 'it', 'they']\n",
            "`original` was updated 77 times in target and 31462 times in context\n",
            "['original', 'construction', 'administration', '1990s']\n",
            "`january` was updated 151 times in target and 48556 times in context\n",
            "['january', 'july', 'december', 'february']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "309987it [09:43, 614.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "avg loss: 30.65385732902176\n",
            "`he` was updated 1747 times in target and 328386 times in context\n",
            "['he', 'she', 'it', 'they']\n",
            "`original` was updated 82 times in target and 32524 times in context\n",
            "['original', 'fourth', 'primary', 'largest']\n",
            "`january` was updated 156 times in target and 50163 times in context\n",
            "['january', 'august', 'november', 'december']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "319996it [10:02, 331.78it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "avg loss: 29.505529299014878\n",
            "`he` was updated 1808 times in target and 338859 times in context\n",
            "['he', 'she', 'it', 'they']\n",
            "`original` was updated 85 times in target and 33617 times in context\n",
            "['original', 'fourth', 'largest', 'pacific']\n",
            "`january` was updated 164 times in target and 51847 times in context\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r320030it [10:02, 192.93it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['january', 'december', 'february', 'july']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "330016it [10:21, 426.12it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "avg loss: 30.7709866916364\n",
            "`he` was updated 1841 times in target and 349312 times in context\n",
            "['he', 'she', 'it', 'they']\n",
            "`original` was updated 87 times in target and 34589 times in context\n",
            "['original', 'fourth', 'japanese', 'largest']\n",
            "`january` was updated 166 times in target and 53443 times in context\n",
            "['january', 'november', 'december', 'february']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "339994it [10:39, 615.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "avg loss: 29.329959593719785\n",
            "`he` was updated 1920 times in target and 360077 times in context\n",
            "['he', 'she', 'they', 'it']\n",
            "`original` was updated 91 times in target and 35644 times in context\n",
            "['original', 'parliament', 'sea', 'nations']\n",
            "`january` was updated 169 times in target and 55032 times in context\n",
            "['january', 'august', 'november', 'july']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "350016it [10:59, 422.13it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "avg loss: 29.73563656224657\n",
            "`he` was updated 1972 times in target and 370536 times in context\n",
            "['he', 'she', 'it', 'they']\n",
            "`original` was updated 93 times in target and 36651 times in context\n",
            "['original', 'security', 'headquarters', 'fourth']\n",
            "`january` was updated 173 times in target and 56702 times in context\n",
            "['january', 'december', 'november', 'august']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "359980it [11:17, 619.30it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "avg loss: 29.3259635888434\n",
            "`he` was updated 2037 times in target and 381336 times in context\n",
            "['he', 'she', 'it', 'they']\n",
            "`original` was updated 94 times in target and 37693 times in context\n",
            "['original', 'security', 'largest', 'sea']\n",
            "`january` was updated 177 times in target and 58330 times in context\n",
            "['january', 'december', 'november', 'august']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "369988it [11:36, 567.18it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "avg loss: 28.879276263005707\n",
            "`he` was updated 2111 times in target and 391925 times in context\n",
            "['he', 'she', 'they', 'it']\n",
            "`original` was updated 95 times in target and 38717 times in context\n",
            "['original', 'security', 'current', 'primary']\n",
            "`january` was updated 183 times in target and 60011 times in context\n",
            "['january', 'december', 'july', 'april']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "379980it [11:54, 617.08it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "avg loss: 30.0203139680596\n",
            "`he` was updated 2156 times in target and 402550 times in context\n",
            "['he', 'she', 'they', 'it']\n",
            "`original` was updated 98 times in target and 39801 times in context\n",
            "['original', 'sea', 'current', 'assembly']\n",
            "`january` was updated 185 times in target and 61598 times in context\n",
            "['january', 'november', 'july', 'september']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "389956it [12:13, 485.44it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "avg loss: 28.45990812292521\n",
            "`he` was updated 2224 times in target and 413177 times in context\n",
            "['he', 'she', 'they', 'it']\n",
            "`original` was updated 99 times in target and 40871 times in context\n",
            "['original', 'official', 'construction', 'office']\n",
            "`january` was updated 191 times in target and 63241 times in context\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "390050it [12:13, 297.68it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['january', 'december', 'july', 'november']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "399982it [12:32, 613.85it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "avg loss: 29.050891767237353\n",
            "`he` was updated 2277 times in target and 423935 times in context\n",
            "['he', 'she', 'they', 'it']\n",
            "`original` was updated 102 times in target and 41939 times in context\n",
            "['original', 'official', 'sea', 'current']\n",
            "`january` was updated 198 times in target and 64831 times in context\n",
            "['january', 'december', 'september', 'august']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "410005it [12:50, 439.96it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "avg loss: 28.58369967786614\n",
            "`he` was updated 2341 times in target and 434423 times in context\n",
            "['he', 'she', 'they', 'it']\n",
            "`original` was updated 105 times in target and 42970 times in context\n",
            "['original', 'current', 'official', 'round']\n",
            "`january` was updated 201 times in target and 66471 times in context\n",
            "['january', 'november', 'december', 'september']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "420028it [13:10, 420.69it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "avg loss: 28.25948376480802\n",
            "`he` was updated 2396 times in target and 445081 times in context\n",
            "['he', 'she', 'they', 'it']\n",
            "`original` was updated 110 times in target and 44081 times in context\n",
            "['original', 'official', 'current', 'head']\n",
            "`january` was updated 204 times in target and 68132 times in context\n",
            "['january', 'december', 'july', 'february']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "430011it [13:28, 429.08it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "avg loss: 29.123622156081314\n",
            "`he` was updated 2450 times in target and 455525 times in context\n",
            "['he', 'she', 'it', 'they']\n",
            "`original` was updated 117 times in target and 45138 times in context\n",
            "['original', 'current', 'sea', 'side']\n",
            "`january` was updated 208 times in target and 69777 times in context\n",
            "['january', 'december', 'march', 'july']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "439991it [13:46, 366.88it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "avg loss: 27.66896580756948\n",
            "`he` was updated 2516 times in target and 466165 times in context\n",
            "['he', 'she', 'it', 'they']\n",
            "`original` was updated 118 times in target and 46141 times in context\n",
            "['original', 'council', 'official', 'largest']\n",
            "`january` was updated 210 times in target and 71367 times in context\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r440029it [13:47, 206.51it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['january', 'december', 'november', 'april']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "450015it [14:06, 422.89it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "avg loss: 28.035358396950482\n",
            "`he` was updated 2565 times in target and 476758 times in context\n",
            "['he', 'she', 'it', 'they']\n",
            "`original` was updated 120 times in target and 47179 times in context\n",
            "['original', 'current', 'official', 'entire']\n",
            "`january` was updated 214 times in target and 72959 times in context\n",
            "['january', 'december', 'august', 'november']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "460000it [14:24, 604.99it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "avg loss: 28.1282208889428\n",
            "`he` was updated 2624 times in target and 487344 times in context\n",
            "['he', 'she', 'it', 'they']\n",
            "`original` was updated 122 times in target and 48195 times in context\n",
            "['original', 'official', 'entire', 'security']\n",
            "`january` was updated 217 times in target and 74602 times in context\n",
            "['january', 'december', 'november', 'march']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "469974it [14:43, 609.33it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "avg loss: 27.460762006894534\n",
            "`he` was updated 2665 times in target and 497842 times in context\n",
            "['he', 'she', 'it', 'they']\n",
            "`original` was updated 125 times in target and 49220 times in context\n",
            "['original', 'official', 'entire', 'largest']\n",
            "`january` was updated 218 times in target and 76245 times in context\n",
            "['january', 'september', 'march', 'june']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "474117it [14:50, 532.33it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3770240967.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0mw2v_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWord2Vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mw2v_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw2v_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-3770240967.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, dataloader)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0;31m# Calculate and store the loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m             \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc_pos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc_neg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0;31m# TODO: Calculate the gradients and implement the gradient update.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "NUM_EPOCHS = 1\n",
        "LEARNING_RATE = 0.01\n",
        "\n",
        "def train(model, dataloader):\n",
        "\n",
        "    num_target_updates = np.zeros((model.target_embs.shape[0],))\n",
        "    num_context_updates = np.zeros((model.context_embs.shape[0],))\n",
        "\n",
        "    def print_diagnostic(word):\n",
        "        print(f\"`{word}` was updated {int(num_target_updates[dataloader.vocab2idx[word]])} times in target and {int(num_context_updates[dataloader.vocab2idx[word]])} times in context\")\n",
        "        model.print_nearest_neighbors(word, 4)\n",
        "\n",
        "    for i in range(NUM_EPOCHS):\n",
        "        losses = []\n",
        "        for i, (target, pos, neg) in enumerate(tqdm(dataloader.sample_contexts(window_size=2, sample_k=100))):\n",
        "\n",
        "            if i % 10_000 == 0:\n",
        "                # Print diagnostic info every 10_000 steps.\n",
        "                print(\"avg loss:\", sum(losses) / len(losses) if losses else \"\")\n",
        "                losses = []\n",
        "                print_diagnostic(\"he\")\n",
        "                print_diagnostic(\"original\")\n",
        "                print_diagnostic(\"january\")\n",
        "\n",
        "            # Get the vectors from the model\n",
        "            w = model.target_embs[target]\n",
        "            c_pos = model.context_embs[pos]\n",
        "            c_neg = model.context_embs[neg]\n",
        "\n",
        "            # Calculate and store the loss\n",
        "            losses.append(loss_fn(w, c_pos, c_neg))\n",
        "\n",
        "            # TODO: Calculate the gradients and implement the gradient update.\n",
        "            # Calculate gradients\n",
        "            grad_w = w_grad(w, c_pos, c_neg)          # (d,)\n",
        "            grad_c_pos = c_pos_grad(w, c_pos)         # (n_pos, d)\n",
        "            grad_c_neg = c_neg_grad(w, c_neg)         # (n_neg, d)\n",
        "\n",
        "            # Update target embedding\n",
        "            model.target_embs[target] -= LEARNING_RATE * grad_w\n",
        "\n",
        "            # Update positive context embeddings\n",
        "            np.subtract.at(model.context_embs, pos, LEARNING_RATE * grad_c_pos)\n",
        "\n",
        "            # Update negative context embeddings\n",
        "            np.subtract.at(model.context_embs, neg, LEARNING_RATE * grad_c_neg)\n",
        "\n",
        "            # Tally up how many times each word has been seen, just for fun.\n",
        "            np.add.at(num_target_updates, target, 1)\n",
        "            np.add.at(num_context_updates, pos, 1)\n",
        "            np.add.at(num_context_updates, neg, 1)\n",
        "\n",
        "w2v_model = Word2Vec(dataloader)\n",
        "model = w2v_model\n",
        "train(w2v_model, dataloader)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8d25f981-30ac-4df4-99af-70c78619dcd7",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "8d25f981-30ac-4df4-99af-70c78619dcd7"
      },
      "source": [
        "Once you are satisfied with the training (you can stop it whenever you want), experiment with printing out some nearest neighbors. Do these align with your expectations? Do any surprise you?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "7b5caadc-5b40-4119-aa7b-075f12ab2055",
      "metadata": {
        "deletable": false,
        "editable": false,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7b5caadc-5b40-4119-aa7b-075f12ab2055",
        "outputId": "2aa9e908-d057-4384-cb4a-c7cb98cb001d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['paris', '1952', 'cash', '1979']\n"
          ]
        }
      ],
      "source": [
        "model.print_nearest_neighbors(\"paris\", 4)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.print_nearest_neighbors(\"january\", 4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f-XtkyhDE0Nb",
        "outputId": "4324d479-1d21-4d8a-e831-5c7cdf1e5499"
      },
      "id": "f-XtkyhDE0Nb",
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['january', 'december', 'august', 'july']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "94ef471a-ff16-4d20-923f-7a5aea813a16",
      "metadata": {
        "id": "94ef471a-ff16-4d20-923f-7a5aea813a16"
      },
      "source": [
        "## Submission"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c000e0b3-1ef4-4ba9-b3b5-cc4690a92ca5",
      "metadata": {
        "id": "c000e0b3-1ef4-4ba9-b3b5-cc4690a92ca5"
      },
      "source": [
        "Congratulations on finishing HW1!\n",
        "\n",
        "Please ensure that you submit a PDF of this notebook onto [Gradescope](https://www.gradescope.com/courses/1238346) before February 3 at 11:59pm."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9589c7fa-68cf-4bee-88b7-78bf92c2dc89",
      "metadata": {
        "id": "9589c7fa-68cf-4bee-88b7-78bf92c2dc89"
      },
      "source": [
        "You can run the cell below to generate a PDF if you are using Google Colab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b252b823-44c7-4d4a-ae50-03cabaac3913",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b252b823-44c7-4d4a-ae50-03cabaac3913",
        "outputId": "0ae0765a-3b7b-431f-d194-8e60b8f1af9d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating PDF. This may take a few seconds.\n"
          ]
        }
      ],
      "source": [
        "#EXPORT_EXCLUDE#\n",
        "\n",
        "#@markdown This is a helper function to generate a PDF in Colab.\n",
        "#@markdown If you are using Jupyter notebook, you can do `File > Save and Export Notebook as HTML`, then save the resulting HTML file as a PDF.\n",
        "#@markdown Alternatively, in Juypter notebook, you might try `File > Save and Export Notebook as PDF`, but just make sure you already have `pandoc` installed.\n",
        "\n",
        "def colab_export_pdf():\n",
        "    # Modified from: https://medium.com/@jonathanagustin/convert-colab-notebook-to-pdf-0ccd8f847dd6\n",
        "    try:\n",
        "        import google.colab\n",
        "        IN_COLAB = True\n",
        "    except:\n",
        "        IN_COLAB = False\n",
        "        print(\"This cell only works in Google Colab!\")\n",
        "        print(\"If you are running locally, click File > Export as HTML. Then open the HTML file and save it as a PDF.\")\n",
        "\n",
        "    if IN_COLAB:\n",
        "        print(\"Generating PDF. This may take a few seconds.\")\n",
        "        import os, datetime, json, locale, pathlib, urllib, requests, werkzeug, nbformat, google, yaml, warnings\n",
        "        locale.setlocale(locale.LC_ALL, 'en_US.UTF-8')\n",
        "        NAME = pathlib.Path(werkzeug.utils.secure_filename(urllib.parse.unquote(requests.get(f\"http://{os.environ['COLAB_JUPYTER_IP']}:{os.environ['KMP_TARGET_PORT']}/api/sessions\").json()[0][\"name\"])))\n",
        "        TEMP = pathlib.Path(\"/content/pdfs\") / f\"{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}_{NAME.stem}\"; TEMP.mkdir(parents=True, exist_ok=True)\n",
        "        NB = [cell for cell in nbformat.reads(json.dumps(google.colab._message.blocking_request(\"get_ipynb\", timeout_sec=30)[\"ipynb\"]), as_version=4).cells if \"--Colab2PDF\" not in cell.source]\n",
        "        warnings.filterwarnings('ignore', category=nbformat.validator.MissingIDFieldWarning)\n",
        "        with (TEMP / f\"{NAME.stem}.ipynb\").open(\"w\", encoding=\"utf-8\") as nb_copy: nbformat.write(nbformat.v4.new_notebook(cells=NB or [nbformat.v4.new_code_cell(\"#\")]), nb_copy)\n",
        "        if not pathlib.Path(\"/usr/local/bin/quarto\").exists():\n",
        "            !wget -q \"https://quarto.org/download/latest/quarto-linux-amd64.deb\" -P {TEMP} && dpkg -i {TEMP}/quarto-linux-amd64.deb > /dev/null && quarto install tinytex --update-path --quiet\n",
        "        with (TEMP / \"config.yml\").open(\"w\", encoding=\"utf-8\") as file: yaml.dump({'include-in-header': [{\"text\": r\"\\usepackage{fvextra}\\DefineVerbatimEnvironment{Highlighting}{Verbatim}{breaksymbolleft={},showspaces=false,showtabs=false,breaklines,breakanywhere,commandchars=\\\\\\{\\}}\"}],'include-before-body': [{\"text\": r\"\\DefineVerbatimEnvironment{verbatim}{Verbatim}{breaksymbolleft={},showspaces=false,showtabs=false,breaklines}\"}]}, file)\n",
        "        !quarto render {TEMP}/{NAME.stem}.ipynb --metadata-file={TEMP}/config.yml --to pdf -M latex-auto-install -M margin-top=1in -M margin-bottom=1in -M margin-left=1in -M margin-right=1in --quiet\n",
        "        google.colab.files.download(str(TEMP / f\"{NAME.stem}.pdf\"))\n",
        "\n",
        "colab_export_pdf()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5682dd69-85ff-4f21-84e6-64ee3146cdb7",
      "metadata": {
        "id": "5682dd69-85ff-4f21-84e6-64ee3146cdb7"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    },
    "otter": {
      "OK_FORMAT": true,
      "tests": {}
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}